{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Differentiation in PyTorch</h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Objective</h2><ul><li> How to perform differentiation in pytorch.</li></ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "<p>In this lab, you will learn the basics of differentiation.</p> \n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#Derivative\">Derivatives</a></li>\n",
    "    <li><a href=\"#Partial_Derivative\">Partial Derivatives</a></li>\n",
    "</ul>\n",
    "\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries will be useing for this lab.\n",
    "\n",
    "import torch \n",
    "import matplotlib.pylab as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Derivative\">Derivatives</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the tensor <code>x</code> and set the parameter <code>requires_grad</code> to true because you are going to take the derivative of the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor x:  tensor(2., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor x\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "print(\"The tensor x: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us create a tensor according to the equation $ y=x^2 $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of y = x^2:  tensor(4., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor y according to y = x^2\n",
    "\n",
    "y = x ** 2\n",
    "print(\"The result of y = x^2: \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us take the derivative with respect x at x = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dervative at x = 2:  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Take the derivative. Try to print out the derivative at the value x = 2\n",
    "\n",
    "y.backward()\n",
    "print(\"The dervative at x = 2: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding lines perform the following operation: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)=4$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor(2.)\n",
      "grad_fn: None\n",
      "grad: tensor(4.)\n",
      "is_leaf: True\n",
      "requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "print('data:',x.data)\n",
    "print('grad_fn:',x.grad_fn)\n",
    "print('grad:',x.grad)\n",
    "print(\"is_leaf:\",x.is_leaf)\n",
    "print(\"requires_grad:\",x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor(4.)\n",
      "grad_fn: <PowBackward0 object at 0x7f20bc41d278>\n",
      "grad: None\n",
      "is_leaf: False\n",
      "requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "print('data:',y.data)\n",
    "print('grad_fn:',y.grad_fn)\n",
    "print('grad:',y.grad)\n",
    "print(\"is_leaf:\",y.is_leaf)\n",
    "print(\"requires_grad:\",y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to calculate the derivative for a more complicated function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of y = x^2 + 2x + 1:  tensor(9., grad_fn=<AddBackward0>)\n",
      "The dervative at x = 2:  tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the y = x^2 + 2x + 1, then find the derivative \n",
    "\n",
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "y = x ** 2 + 2 * x + 1\n",
    "print(\"The result of y = x^2 + 2x + 1: \", y)\n",
    "y.backward()\n",
    "print(\"The dervative at x = 2: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is in the following form:\n",
    "$y=x^{2}+2x+1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x+2$\n",
    "\n",
    "$\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)+2=6$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the derivative of $ y = 2x^3+x $ at $x=1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative result:  tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "# Practice: Calculate the derivative of y = 2x^3 + x at x = 1\n",
    "\n",
    "# Type your code here\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = 2 * x ** 3 + x\n",
    "y.backward()\n",
    "print(\"The derivative result: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click <b>here</b> for the solution.\n",
    "\n",
    "<!-- \n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = 2 * x ** 3 + x\n",
    "y.backward()\n",
    "print(\"The derivative result: \", x.grad)\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQ(torch.autograd.Function):\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx,i):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        result=i**2\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        i, = ctx.saved_tensors\n",
    "        grad_output = 2*i\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply it the function  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.SQBackward object at 0x7f1fcabaee48>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor(2.0,requires_grad=True )\n",
    "sq=SQ.apply\n",
    "\n",
    "y=sq(x)\n",
    "y\n",
    "print(y.grad_fn)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Partial_Derivative\">Partial Derivatives</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate <b>Partial Derivatives</b>. Consider the function: $f(u,v)=vu+u^{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create <code>u</code> tensor, <code>v</code> tensor and  <code>f</code> tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of v * u + u^2:  tensor(3., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate f(u, v) = v * u + u^2 at u = 1, v = 2\n",
    "\n",
    "u = torch.tensor(1.0,requires_grad=True)\n",
    "v = torch.tensor(2.0,requires_grad=True)\n",
    "f = u * v + u ** 2\n",
    "print(\"The result of v * u + u^2: \", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to the following: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(u=1,v=2)=(2)(1)+1^{2}=3$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take the derivative with respect to <code>u</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The partial derivative with respect to u:  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the derivative with respect to u\n",
    "\n",
    "f.backward()\n",
    "print(\"The partial derivative with respect to u: \", u.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the expression is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {u}}=v+2u$\n",
    "\n",
    "$\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {u}}=2+2(1)=4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take the derivative with respect to <code>v</code>:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The partial derivative with respect to u:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the derivative with respect to v\n",
    "\n",
    "print(\"The partial derivative with respect to u: \", v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation is given by:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {v}}=u$\n",
    "\n",
    "$\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {v}}=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the derivative with respect to a function with multiple values as follows. You use the sum trick to produce a scalar valued function and then take the gradient: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.0000,  -7.7778,  -5.5556,  -3.3333,  -1.1111,   1.1111,   3.3333,\n",
      "          5.5556,   7.7778,  10.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the derivative with multiple values\n",
    "\n",
    "x = torch.linspace(-10, 10, 10, requires_grad = True)\n",
    "Y = x ** 2\n",
    "y = torch.sum(x ** 2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the function  and its derivative \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7b5202ef1a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Take the derivative with respect to multiple value. Plot out the function and its derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# Take the derivative with respect to multiple value. Plot out the function and its derivative\n",
    "\n",
    "y.backward()\n",
    "\n",
    "\n",
    "plt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\n",
    "plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange line is the slope of the blue line at the intersection point, which is the derivative of the blue line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  method <code> detach()</code>  excludes further tracking of operations in the graph, and therefore the subgraph will not record operations. This allows us to then convert the tensor to a numpy array. To understand the sum operation  <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\">Click Here</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>relu</b> activation function is an essential function in neural networks. We can take the derivative as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the derivative of Relu with respect to multiple value. Plot out the function and its derivative\n",
    "\n",
    "x = torch.linspace(-10, 10, 1000, requires_grad = True)\n",
    "Y = torch.relu(x)\n",
    "y = Y.sum()\n",
    "y.backward()\n",
    "plt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\n",
    "plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x7f1fc866ce48>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Practice</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to determine partial derivative  $u$ of the following function where $u=2$ and $v=1$: $ f=uv+(uv)^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Calculate the derivative of f = u * v + (u * v) ** 2 at u = 2, v = 1\n",
    "\n",
    "# Type the code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "\n",
    "<!-- \n",
    "u = torch.tensor(2.0, requires_grad = True)\n",
    "v = torch.tensor(1.0, requires_grad = True)\n",
    "f = u * v + (u * v) ** 2\n",
    "f.backward()\n",
    "print(\"The result is \", u.grad)\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?context=cpdaas&apps=data_science_experience,watson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
